{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from stable_audio_tools import create_model_from_config\n",
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from stable_audio_tools.training.training_wrapper import DiffusionCondTrainingWrapper\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "\n",
    "model_config_file = './stable_audio_tools/configs/model_config.json'\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['conditioner.conditioners.duration.embedder.embedding.0.weights', 'conditioner.conditioners.duration.embedder.embedding.1.weight', 'conditioner.conditioners.duration.embedder.embedding.1.bias'], unexpected_keys=['conditioner.conditioners.seconds_start.embedder.embedding.0.weights', 'conditioner.conditioners.seconds_start.embedder.embedding.1.bias', 'conditioner.conditioners.seconds_start.embedder.embedding.1.weight', 'conditioner.conditioners.seconds_total.embedder.embedding.0.weights', 'conditioner.conditioners.seconds_total.embedder.embedding.1.bias', 'conditioner.conditioners.seconds_total.embedder.embedding.1.weight'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model_from_config(model_config)\n",
    "model.load_state_dict(load_file('./weight/StableAudio/model.safetensors'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22050 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dirs = ['./dataset/feature/train/AudioSet/10']\n",
    "audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10']\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':44100, \n",
    "    'force_channels':\"stereo\"\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':10, \n",
    "    'shuffle':True,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = model_config.get('training', None)\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "            model, \n",
    "            lr=training_config.get(\"learning_rate\", None),\n",
    "            mask_padding=training_config.get(\"mask_padding\", False),\n",
    "            mask_padding_dropout=training_config.get(\"mask_padding_dropout\", 0.0),\n",
    "            use_ema = training_config.get(\"use_ema\", True),\n",
    "            log_loss_info=training_config.get(\"log_loss_info\", False),\n",
    "            optimizer_configs=training_config.get(\"optimizer_configs\", None),\n",
    "            pre_encoded=training_config.get(\"pre_encoded\", False),\n",
    "            cfg_dropout_prob = training_config.get(\"cfg_dropout_prob\", 0.1),\n",
    "            timestep_sampler = training_config.get(\"timestep_sampler\", \"uniform\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     devices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      3\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:107\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Launches processes that run the given function in parallel.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mThe function is allowed to have a return value. However, when all processes join, only the return value\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 107\u001b[0m     \u001b[43m_check_bad_cuda_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    109\u001b[0m     _check_missing_main_guard()\n",
      "File \u001b[0;32m~/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/lightning_fabric/strategies/launchers/multiprocessing.py:205\u001b[0m, in \u001b[0;36m_check_bad_cuda_fork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE:\n\u001b[1;32m    204\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You will have to restart the Python kernel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/selectors.py\", line 468, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 2462961) is killed by signal: Terminated. \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=[1],\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20280 files\n"
     ]
    }
   ],
   "source": [
    "info_dirs = ['./dataset/feature/test/AudioSet/10']\n",
    "audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/test/10']\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':44100, \n",
    "    'force_channels':\"stereo\"\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':10, \n",
    "    # 'shuffle':True,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3762202933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torchsde/_brownian/brownian_interval.py:608: UserWarning: Should have tb<=t1 but got tb=500.00006103515625 and t1=500.000061.\n",
      "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset/video/test/AudioSet/10/--4gqARaEJE_p.mp4\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "sample_size = 441000\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "for audio, conditioning in dataloader:\n",
    "    output = generate_diffusion_cond(\n",
    "        model,\n",
    "        steps=100,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=sample_size,\n",
    "        batch_size=dl_config['batch_size'],\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"dpmpp-3m-sde\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Rearrange audio batch to a single sequence\n",
    "    idx = 0\n",
    "    output = output[idx:1+idx]\n",
    "    output = rearrange(output, \"b d n -> d (b n)\")\n",
    "\n",
    "    output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "    torchaudio.save(\"output.wav\", output, sample_rate)\n",
    "    print(conditioning['video_path'][idx])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_audio_tools.models.diffusion import DiTWrapper, ConditionedDiffusionModelWrapper\n",
    "from stable_audio_tools.models.conditioners import create_multi_conditioner_from_conditioning_config\n",
    "from stable_audio_tools.models.autoencoders import create_autoencoder_from_config\n",
    "from stable_audio_tools.models.pretransforms import AutoencoderPretransform\n",
    "\n",
    "io_channels = model_config[\"model\"].get('io_channels', None)\n",
    "sample_rate = model_config.get('sample_rate', None)\n",
    "assert io_channels is not None, \"Must specify io_channels in model config\"\n",
    "assert sample_rate is not None, \"Must specify sample_rate in config\"\n",
    "\n",
    "\n",
    "\n",
    "diffusion_config = {\n",
    "    'cross_attention_cond_ids': ['feature'],\n",
    "    'type': 'dit',\n",
    "    'config': {\n",
    "        'io_channels': 64,\n",
    "        'embed_dim': 1536,\n",
    "        'depth': 24,\n",
    "        'num_heads': 24,\n",
    "        'cond_token_dim': 768,\n",
    "        'global_cond_dim': 1536,\n",
    "        'project_cond_tokens': False,\n",
    "        'transformer_type': 'continuous_transformer'\n",
    "        }\n",
    "}\n",
    "diffusion_objective = diffusion_config.get('diffusion_objective', 'v')\n",
    "cross_attention_ids = diffusion_config.get('cross_attention_cond_ids', [])\n",
    "global_cond_ids = diffusion_config.get('global_cond_ids', [])\n",
    "input_concat_ids = diffusion_config.get('input_concat_ids', [])\n",
    "prepend_cond_ids = diffusion_config.get('prepend_cond_ids', [])\n",
    "\n",
    "diffusion_model_config = diffusion_config['config']\n",
    "diffusion_model = DiTWrapper(**diffusion_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning_config = {\n",
    "    'configs': [\n",
    "        {\n",
    "            'id': 'duration',\n",
    "            'type': 'number',\n",
    "            'config': {'min_val': 0, 'max_val': 512}\n",
    "         },\n",
    "         {\n",
    "             'id': 'feature', \n",
    "             'type': 'video_feature', \n",
    "             'config': {}\n",
    "        }\n",
    "    ],\n",
    "    'cond_dim': 768,\n",
    "    'default_keys': {'feature': 'video_path'}\n",
    "}\n",
    "conditioner = create_multi_conditioner_from_conditioning_config(conditioning_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretransform_config = {\n",
    "    'type': 'autoencoder',\n",
    "    'iterate_batch': True,\n",
    "    'config': {\n",
    "        'encoder': {\n",
    "            'type': 'oobleck',\n",
    "            'requires_grad': False,\n",
    "            'config': {\n",
    "                'in_channels': 2,\n",
    "                'channels': 128,\n",
    "                'c_mults': [1, 2, 4, 8, 16],\n",
    "                'strides': [2, 4, 4, 8, 8],\n",
    "                'latent_dim': 128,\n",
    "                'use_snake': True\n",
    "            }\n",
    "        },\n",
    "        'decoder': {\n",
    "            'type': 'oobleck',\n",
    "            'config': {\n",
    "                'out_channels': 2,\n",
    "                'channels': 128,\n",
    "                'c_mults': [1, 2, 4, 8, 16],\n",
    "                'strides': [2, 4, 4, 8, 8],\n",
    "                'latent_dim': 64,\n",
    "                'use_snake': True,\n",
    "                'final_tanh': False\n",
    "            }\n",
    "        },\n",
    "        'bottleneck': {'type': 'vae'},\n",
    "        'latent_dim': 64,\n",
    "        'downsampling_ratio': 2048,\n",
    "        'io_channels': 2\n",
    "        }\n",
    "    }\n",
    "\n",
    "autoencoder_config = {\"sample_rate\": sample_rate, \"model\": pretransform_config[\"config\"]}\n",
    "autoencoder = create_autoencoder_from_config(autoencoder_config)\n",
    "\n",
    "\n",
    "pretransform = AutoencoderPretransform(\n",
    "    autoencoder, \n",
    "    scale=pretransform_config.get(\"scale\", 1.0),\n",
    "    model_half=pretransform_config.get(\"model_half\", False), \n",
    "    iterate_batch=pretransform_config.get(\"iterate_batch\", False), \n",
    "    chunked=pretransform_config.get(\"chunked\", False)\n",
    ")\n",
    "pretransform.enable_grad = pretransform_config.get('enable_grad', False)\n",
    "pretransform.eval().requires_grad_(pretransform.enable_grad)\n",
    "\n",
    "\n",
    "min_input_length = pretransform.downsampling_ratio\n",
    "min_input_length *= diffusion_model.model.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kwargs = {\"diffusion_objective\":diffusion_objective}\n",
    "model = ConditionedDiffusionModelWrapper(\n",
    "        diffusion_model,\n",
    "        conditioner,\n",
    "        min_input_length=min_input_length,\n",
    "        sample_rate=sample_rate,\n",
    "        cross_attn_cond_ids=cross_attention_ids,\n",
    "        global_cond_ids=global_cond_ids,\n",
    "        input_concat_ids=input_concat_ids,\n",
    "        prepend_cond_ids=prepend_cond_ids,\n",
    "        pretransform=pretransform,\n",
    "        io_channels=io_channels,\n",
    "        **extra_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

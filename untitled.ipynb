{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "\n",
    "from stable_audio_tools import create_model_from_config\n",
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from stable_audio_tools.training.training_wrapper import DiffusionCondTrainingWrapper\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "model_config_file = '/home/chengxin/chengxin/stable-v2a/stable_audio_tools/configs/model_config.json'\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['conditioner.conditioners.duration.embedder.embedding.0.weights', 'conditioner.conditioners.duration.embedder.embedding.1.weight', 'conditioner.conditioners.duration.embedder.embedding.1.bias'], unexpected_keys=['conditioner.conditioners.seconds_start.embedder.embedding.0.weights', 'conditioner.conditioners.seconds_start.embedder.embedding.1.bias', 'conditioner.conditioners.seconds_start.embedder.embedding.1.weight', 'conditioner.conditioners.seconds_total.embedder.embedding.0.weights', 'conditioner.conditioners.seconds_total.embedder.embedding.1.bias', 'conditioner.conditioners.seconds_total.embedder.embedding.1.weight'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model_from_config(model_config)\n",
    "model.load_state_dict(load_file('../stable-v2a/dataset/StableAudio/model.safetensors'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 files\n"
     ]
    }
   ],
   "source": [
    "info_dirs = ['/home/chengxin/chengxin/stable-v2a/dataset/video_feature/test/AudioSet']\n",
    "audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/test/10']\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':44100, \n",
    "    'force_channels':\"stereo\"\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':16, \n",
    "    'shuffle':True,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_audio_tools.models.diffusion import DiTWrapper, ConditionedDiffusionModelWrapper\n",
    "from stable_audio_tools.models.conditioners import create_multi_conditioner_from_conditioning_config\n",
    "from stable_audio_tools.models.autoencoders import create_autoencoder_from_config\n",
    "from stable_audio_tools.models.pretransforms import AutoencoderPretransform\n",
    "\n",
    "io_channels = model_config[\"model\"].get('io_channels', None)\n",
    "sample_rate = model_config.get('sample_rate', None)\n",
    "assert io_channels is not None, \"Must specify io_channels in model config\"\n",
    "assert sample_rate is not None, \"Must specify sample_rate in config\"\n",
    "\n",
    "\n",
    "\n",
    "diffusion_config = {\n",
    "    'cross_attention_cond_ids': ['feature'],\n",
    "    'type': 'dit',\n",
    "    'config': {\n",
    "        'io_channels': 64,\n",
    "        'embed_dim': 1536,\n",
    "        'depth': 24,\n",
    "        'num_heads': 24,\n",
    "        'cond_token_dim': 768,\n",
    "        'global_cond_dim': 1536,\n",
    "        'project_cond_tokens': False,\n",
    "        'transformer_type': 'continuous_transformer'\n",
    "        }\n",
    "}\n",
    "diffusion_objective = diffusion_config.get('diffusion_objective', 'v')\n",
    "cross_attention_ids = diffusion_config.get('cross_attention_cond_ids', [])\n",
    "global_cond_ids = diffusion_config.get('global_cond_ids', [])\n",
    "input_concat_ids = diffusion_config.get('input_concat_ids', [])\n",
    "prepend_cond_ids = diffusion_config.get('prepend_cond_ids', [])\n",
    "\n",
    "diffusion_model_config = diffusion_config['config']\n",
    "diffusion_model = DiTWrapper(**diffusion_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning_config = {\n",
    "    'configs': [\n",
    "        {\n",
    "            'id': 'duration',\n",
    "            'type': 'number',\n",
    "            'config': {'min_val': 0, 'max_val': 512}\n",
    "         },\n",
    "         {\n",
    "             'id': 'feature', \n",
    "             'type': 'video_feature', \n",
    "             'config': {}\n",
    "        }\n",
    "    ],\n",
    "    'cond_dim': 768,\n",
    "    'default_keys': {'feature': 'video_path'}\n",
    "}\n",
    "conditioner = create_multi_conditioner_from_conditioning_config(conditioning_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretransform_config = {\n",
    "    'type': 'autoencoder',\n",
    "    'iterate_batch': True,\n",
    "    'config': {\n",
    "        'encoder': {\n",
    "            'type': 'oobleck',\n",
    "            'requires_grad': False,\n",
    "            'config': {\n",
    "                'in_channels': 2,\n",
    "                'channels': 128,\n",
    "                'c_mults': [1, 2, 4, 8, 16],\n",
    "                'strides': [2, 4, 4, 8, 8],\n",
    "                'latent_dim': 128,\n",
    "                'use_snake': True\n",
    "            }\n",
    "        },\n",
    "        'decoder': {\n",
    "            'type': 'oobleck',\n",
    "            'config': {\n",
    "                'out_channels': 2,\n",
    "                'channels': 128,\n",
    "                'c_mults': [1, 2, 4, 8, 16],\n",
    "                'strides': [2, 4, 4, 8, 8],\n",
    "                'latent_dim': 64,\n",
    "                'use_snake': True,\n",
    "                'final_tanh': False\n",
    "            }\n",
    "        },\n",
    "        'bottleneck': {'type': 'vae'},\n",
    "        'latent_dim': 64,\n",
    "        'downsampling_ratio': 2048,\n",
    "        'io_channels': 2\n",
    "        }\n",
    "    }\n",
    "\n",
    "autoencoder_config = {\"sample_rate\": sample_rate, \"model\": pretransform_config[\"config\"]}\n",
    "autoencoder = create_autoencoder_from_config(autoencoder_config)\n",
    "\n",
    "\n",
    "pretransform = AutoencoderPretransform(\n",
    "    autoencoder, \n",
    "    scale=pretransform_config.get(\"scale\", 1.0),\n",
    "    model_half=pretransform_config.get(\"model_half\", False), \n",
    "    iterate_batch=pretransform_config.get(\"iterate_batch\", False), \n",
    "    chunked=pretransform_config.get(\"chunked\", False)\n",
    ")\n",
    "pretransform.enable_grad = pretransform_config.get('enable_grad', False)\n",
    "pretransform.eval().requires_grad_(pretransform.enable_grad)\n",
    "\n",
    "\n",
    "min_input_length = pretransform.downsampling_ratio\n",
    "min_input_length *= diffusion_model.model.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kwargs = {\"diffusion_objective\":diffusion_objective}\n",
    "model = ConditionedDiffusionModelWrapper(\n",
    "        diffusion_model,\n",
    "        conditioner,\n",
    "        min_input_length=min_input_length,\n",
    "        sample_rate=sample_rate,\n",
    "        cross_attn_cond_ids=cross_attention_ids,\n",
    "        global_cond_ids=global_cond_ids,\n",
    "        input_concat_ids=input_concat_ids,\n",
    "        prepend_cond_ids=prepend_cond_ids,\n",
    "        pretransform=pretransform,\n",
    "        io_channels=io_channels,\n",
    "        **extra_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = model_config.get('training', None)\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "            model, \n",
    "            lr=training_config.get(\"learning_rate\", None),\n",
    "            mask_padding=training_config.get(\"mask_padding\", False),\n",
    "            mask_padding_dropout=training_config.get(\"mask_padding_dropout\", 0.0),\n",
    "            use_ema = training_config.get(\"use_ema\", True),\n",
    "            log_loss_info=training_config.get(\"log_loss_info\", False),\n",
    "            optimizer_configs=training_config.get(\"optimizer_configs\", None),\n",
    "            pre_encoded=training_config.get(\"pre_encoded\", False),\n",
    "            cfg_dropout_prob = training_config.get(\"cfg_dropout_prob\", 0.1),\n",
    "            timestep_sampler = training_config.get(\"timestep_sampler\", \"uniform\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name          | Type                             | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | diffusion     | ConditionedDiffusionModelWrapper | 1.2 B \n",
      "1 | diffusion_ema | EMA                              | 1.1 B \n",
      "2 | losses        | MultiLoss                        | 0     \n",
      "-------------------------------------------------------------------\n",
      "1.1 B     Trainable params\n",
      "1.2 B     Non-trainable params\n",
      "2.3 B     Total params\n",
      "9,079.871 Total estimated model params size (MB)\n",
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, v_num=2, train/loss=0.994, train/std_data=1.010, train/lr=1.97e-6, train/mse_loss=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=10000000,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1323000, 44100, 30.0, 30, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import load_file\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import safetensors\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_audio_tools import create_model_from_config, replace_audio, save_audio\n",
    "from stable_audio_tools.data.dataset import VideoFeatDataset, VideoFeatDataset_VL, collation_fn\n",
    "from stable_audio_tools.training.training_wrapper import DiffusionCondTrainingWrapper\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond, generate_diffusion_cond_from_path\n",
    "\n",
    "\n",
    "model_config_file = './stable_audio_tools/configs/model_config_gc16000_vl30.json'\n",
    "model_config_file = './stable_audio_tools/configs/model_config_vl30.json'\n",
    "\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)\n",
    "    sample_rate = model_config[\"sample_rate\"]\n",
    "    sample_size = model_config[\"sample_size\"]\n",
    "    fps = model_config[\"fps\"]\n",
    "    variable_length = model_config[\"variable_length\"]\n",
    "    force_channels = \"stereo\" if model_config[\"audio_channels\"] == 2 else \"mono\"\n",
    "    # sample_size = None\n",
    "    # variable_length = None\n",
    "\n",
    "\n",
    "model = create_model_from_config(model_config)\n",
    "sample_size, sample_rate, sample_size/sample_rate, variable_length, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./weight/StableAudio/2024-08-24 20:27:19/epoch=0-step=1409.safetensors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = './weight/StableAudio/2024-08-24 20:27:19'\n",
    "model_name = 'epoch=0-step=1409'  \n",
    "# ./weight/StableAudio/lightning_logs/version_3/checkpoints  epoch=9-step=14620                                     FPS=22  config\n",
    "# ./weight/StableAudio/2024-07-06 10:28:13               在basemodel的基础上 加入pos_emb,利用AS进行训练<epoch=30-step=58>   time_align和生成效果表现较好(best)   FPS=22 config\n",
    "# ./weight/StableAudio/2024-07-20 13:34:51               在basemodel的基础上 加入pos_emb 利用VGG重新训练 epoch=3-step=1817                          FPS=22  config\n",
    "# ./weight/StableAudio/2024-07-22 19:08:45               在不load t2a-crosscond&conditioner的基础上 加入rotary_cond_emb 利用AS重新训练              FPS=8 sr=16000  config_rotebd\n",
    "# ./weight/StableAudio/2024-07-24 23:06:33               在bestmodel的基础上    利用AS继续进行训练     <epoch=70-step=304> <epoch=150-step=304>     FPS=22  config\n",
    "# ./weight/StableAudio/2024-08-01 09:36:20               在lastmodel<epoch=70-step=304>的基础上    利用ASVGG继续进行训练   <epoch=45-step=2818>  <epoch=29-step=2818>    FPS=22  config\n",
    "# ./weight/StableAudio/2024-08-04 02:52:24               在lastmodel<epoch=29-step=2818>的基础上    利用ASVGG继续进行训练  <epoch=27-step=2818> <epoch=36-step=2818> <epoch=60-step=2818>\n",
    "# ./weight/StableAudio/2024-08-15 11:31:46               在lastmodel<epoch=36-step=2818>的基础上 加入global_cond_ids [\"seconds_start\", \"seconds_total\"] samplesize44100*30 variablelength30\n",
    "# ./weight/StableAudio/2024-08-19 11:11:56               在lastmodel<epoch=36-step=2818>, sr16000 ss160000 fps8 noclamp  加入global_cond_ids 利用VGG继续进行训练   <epoch=60-step=2818> <epoch=30-step=2818> <epoch=35-step=2818> \n",
    "\n",
    "try:\n",
    "    state_dict = load_file(f'{model_dir}/{model_name}.safetensors')\n",
    "except:\n",
    "    state_dict = torch.load(f'{model_dir}/{model_name}.ckpt')['state_dict']\n",
    "    state_dict = OrderedDict([(\".\".join(key.split('.')[1:]), value)  for key, value in state_dict.items()])\n",
    "    safetensors.torch.save_file(state_dict, f'{model_dir}/{model_name}.safetensors')\n",
    "\n",
    "print(f'{model_dir}/{model_name}.safetensors')\n",
    "\n",
    "# cond_state_dict = load_file(f'./weight/StableAudio/model.safetensors')\n",
    "# cond_state_dict = {k: v for k, v in cond_state_dict.items() if ('seconds' in k)}\n",
    "# state_dict.update(cond_state_dict)\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([64,1])\n",
    "model.model.model.transformer.pos_emb_cond(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 T2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 6\n",
    "state_dict = load_file(f'./weight/StableAudio/model.safetensors')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up text and timing conditioning\n",
    "conditioning = {\n",
    "    \"prompt\": [\"laugh\"], ### 128 BPM tech house drum loop\n",
    "    # 'feature': ['/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4'],\n",
    "    \"seconds_start\": torch.tensor([0]), \n",
    "    \"seconds_total\": torch.tensor([10])\n",
    "}\n",
    "\n",
    "# Generate stereo audio\n",
    "output = generate_diffusion_cond(\n",
    "    model,\n",
    "    steps=50,\n",
    "    cfg_scale=7,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=44100*47,\n",
    "    sigma_min=0.3,\n",
    "    sigma_max=500,\n",
    "    sampler_type=\"k-dpm-fast\", # dpmpp-3m-sde\n",
    "    device=device,\n",
    "    seed=3044415654\n",
    ")\n",
    "# Rearrange audio batch to a single sequence\n",
    "output = rearrange(output, \"b d n -> d (b n)\")\n",
    "\n",
    "# Peak normalize, clip, convert to int16, and save to file\n",
    "output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "torchaudio.save(\"output.wav\", output, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 files\n"
     ]
    }
   ],
   "source": [
    "info_dirs = [\n",
    "    # './dataset/feature/train/AudioSet/10', \n",
    "    './dataset/feature/train/VGGSound/10',\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    # '/home/chengxin/chengxin/AudioSet/generated_audios/train/10', \n",
    "    '/home/chengxin/chengxin/VGGSound/generated_audios/train/10'\n",
    "    ]\n",
    "# info_dirs = ['./dataset/feature/train/AudioSet/10', './dataset/feature/train/VGGSound/10']\n",
    "# audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10', '/home/chengxin/chengxin/VGGSound/generated_audios/train/10']\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'sample_size':sample_size,\n",
    "    'variable_length':variable_length,\n",
    "    'fps':fps,\n",
    "    # 'force_channels':\"mono\",\n",
    "    'limit_num':16\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':4, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset_VL(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | diffusion | ConditionedDiffusionModelWrapper | 1.2 B \n",
      "1 | losses    | MultiLoss                        | 0     \n",
      "---------------------------------------------------------------\n",
      "1.1 B     Trainable params\n",
      "156 M     Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,880.998 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s, v_num=5]Epoch:0   Loss:0.7678540945053101\n"
     ]
    }
   ],
   "source": [
    "training_config = model_config.get('training', None)\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "            model=model, \n",
    "            lr=training_config.get(\"learning_rate\", None),\n",
    "            optimizer_configs=training_config.get(\"optimizer_configs\", None),\n",
    "            pre_encoded=training_config.get(\"pre_encoded\", False),\n",
    "            cfg_dropout_prob = training_config.get(\"cfg_dropout_prob\", 0.1),\n",
    "            timestep_sampler = training_config.get(\"timestep_sampler\", \"uniform\"),\n",
    "            # duration_mask=None,\n",
    "            latent_per_sec = sample_rate // 2000\n",
    "        )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=[7],\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=1\n",
    ")\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "info_dirs = [\n",
    "    # './dataset/feature/test/VGGSound/10', \n",
    "    './dataset/feature/test/unav100/10'\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    # '/home/chengxin/chengxin/VGGSound/generated_audios/test/10', \n",
    "    '/home/chengxin/chengxin/unav100/generated_audios/test/10'\n",
    "    ]\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'sample_size':sample_size,\n",
    "    'variable_length':variable_length,\n",
    "    'fps':fps,\n",
    "    # 'force_channels':\"mono\",\n",
    "    'limit_num':4\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':4, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset_VL(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./demo/epoch=36-step=2818\n",
      "2653045574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:10<00:00, 14.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_dir = './weight/StableAudio/2024-07-06 10:28:13'\n",
    "# model_name = 'epoch=30-step=58'  \n",
    "\n",
    "device = 0\n",
    "output_dir = f\"./demo/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "for audio, conditioning in dataloader:\n",
    "    # print(int(sample_rate*seconds_total), 44100*60)\n",
    "    sample_size_ = int(sample_rate*max(conditioning['seconds_total'])) if sample_size == None else sample_size\n",
    "    output = generate_diffusion_cond(\n",
    "        model = model.to(device),\n",
    "        steps=150,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=sample_size_,\n",
    "        batch_size=len(conditioning['feature']),\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"dpmpp-3m-sde\", # k-dpm-fast\"\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    for idx in range(len(conditioning['feature'])):\n",
    "        # Save generated audio\n",
    "        if 'AuidoSet' in conditioning['video_path'][idx] or 'AudioSet' in conditioning['video_path'][idx]:\n",
    "            l = conditioning['video_path'][idx].split('/')\n",
    "            video_path = os.path.join('/home/chengxin/chengxin/AudioSet/dataset/', l[-4], l[-2], l[-1])\n",
    "        else:\n",
    "            video_path = conditioning['video_path'][idx].replace('../../../', '/home/chengxin/chengxin/')\n",
    "\n",
    "        # video_path = conditioning['video_path'][idx].replace('../../', './')\n",
    "        audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "        audio_path = f\"./asset/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "        waveform = output[idx:1+idx,...,:int(conditioning['seconds_total'][idx]*sample_rate)]\n",
    "        # print(output.shape, output[idx:idx+1].shape, waveform.shape, conditioning['seconds_total'][idx])\n",
    "        save_audio(waveform, audio_path, sample_rate)\n",
    "        \n",
    "        # Replace the audio of original video to generated one\n",
    "        moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "        shutil.copy(video_path, moved_video_path)\n",
    "        generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "        replace_audio(moved_video_path, audio_path, generated_video_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3531005182\n",
      "Extracting features from video:/home/chengxin/chengxin/unav100/dataset/test/-JtpqwcTYGA.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:04<00:00, 31.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "device = 0\n",
    "conditioning = {\n",
    "    # 'seconds_start': [0, 0],\n",
    "    # 'seconds_total': [10, 10],\n",
    "    'feature': ['/home/chengxin/chengxin/unav100/dataset/test/-JtpqwcTYGA.mp4']\n",
    "}\n",
    "video_clip = VideoFileClip(conditioning['feature'][0])\n",
    "seconds_total = int(video_clip.duration)\n",
    "seconds_total = 10\n",
    "\n",
    "output = generate_diffusion_cond(\n",
    "    model = model.to(device),\n",
    "    steps=150,\n",
    "    cfg_scale=7,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=int(sample_rate*seconds_total),\n",
    "    batch_size=len(conditioning['feature']),\n",
    "    sigma_min=0.3,\n",
    "    sigma_max=500,\n",
    "    sampler_type=\"dpmpp-3m-sde\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "audio_path = \"./test.wav\"\n",
    "waveform = output[0:1,...,:int(seconds_total*sample_rate)]\n",
    "# print(output.shape, output[idx:idx+1].shape, waveform.shape)\n",
    "save_audio(waveform, audio_path, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 0\n",
    "# seconds_total = 10\n",
    "# video_paths = ['/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4', '/home/chengxin/chengxin/FoleyCrafter/examples/sora/1.mp4']\n",
    "# output_dir = \"./demo\"\n",
    "\n",
    "# conditioning = {'seconds_start':0,'seconds_total':10}\n",
    "# output = generate_diffusion_cond_from_path(\n",
    "#     model=model.to(device),\n",
    "#     video_paths=video_paths,\n",
    "#     conditioning=conditioning,\n",
    "#     steps=100,\n",
    "#     cfg_scale=7,\n",
    "#     sample_rate=sample_rate,\n",
    "#     sample_size=sample_rate*seconds_total,\n",
    "#     batch_size=len(video_paths),\n",
    "#     sigma_min=0.3,\n",
    "#     sigma_max=500,\n",
    "#     sampler_type=\"dpmpp-3m-sde\",\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# for idx in range(len(video_paths)):\n",
    "#     # Save generated audio\n",
    "#     video_path = video_paths[idx]\n",
    "#     audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "#     save_audio(output[idx:1+idx], audio_path, sample_rate)\n",
    "        \n",
    "#     # Replace the audio of original video to generated one\n",
    "#     moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "#     shutil.copy(video_path, moved_video_path)\n",
    "#     generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "#     replace_audio(moved_video_path, audio_path, generated_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Time Cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "info_dirs = [\n",
    "    './dataset/feature_t/test/AudioSet/10', \n",
    "    # './dataset/feature/test/unav100/10'\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    '/home/chengxin/chengxin/AudioSet/generated_audios/test/10', \n",
    "    # '/home/chengxin/chengxin/unav100/generated_audios/test'\n",
    "    ]\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    # 'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'fps':fps,\n",
    "    'force_channels':\"mono\",\n",
    "    'limit_num':32\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':32, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./demo/epoch=30-step=58\n",
      "3642395041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.92it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir = './weight/StableAudio/2024-07-06 10:28:13'\n",
    "model_name = 'epoch=30-step=58'  \n",
    "\n",
    "device = 7\n",
    "output_dir = f\"./demo/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "for conditioning in dataloader:\n",
    "    del conditioning['seconds_start']\n",
    "    seconds_total = max(conditioning['seconds_total'])\n",
    "    output = generate_diffusion_cond(\n",
    "        model = model.to(device),\n",
    "        steps=50,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=int(sample_rate*seconds_total),\n",
    "        batch_size=len(conditioning['feature']),\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"k-dpm-fast\",\n",
    "        device=device\n",
    "    )\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

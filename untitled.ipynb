{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44100, 22, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import safetensors\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_audio_tools import create_model_from_config, replace_audio, save_audio\n",
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from stable_audio_tools.training.training_wrapper import DiffusionCondTrainingWrapper\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond, generate_diffusion_cond_from_path\n",
    "\n",
    "\n",
    "model_config_file = './stable_audio_tools/configs/model_config.json'\n",
    "# config_t2a: t2a      config:v2a     config1:new v2a\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "fps = model_config[\"fps\"]\n",
    "sample_size = None\n",
    "\n",
    "model = create_model_from_config(model_config)\n",
    "sample_rate, fps, sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./weight/StableAudio/2024-08-04 02:52:24/epoch=27-step=2818.safetensors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = './weight/StableAudio/2024-08-04 02:52:24'\n",
    "model_name = 'epoch=27-step=2818'  \n",
    "# ./weight/StableAudio/lightning_logs/version_3/checkpoints  epoch=9-step=14620                                     FPS=22  config\n",
    "# ./weight/StableAudio/2024-07-02 19:49:04                   epoch=2-step=427                                       FPS=22 basemodel config\n",
    "# ./weight/StableAudio/2024-07-06 10:28:13               在basemodel的基础上 加入pos_emb,利用AS进行训练<epoch=30-step=58>   time_align和生成效果表现较好(best)   FPS=22 config\n",
    "# ./weight/StableAudio/2024-07-06 21:34:56               在bestmodel的基础上    加入pos_emb 利用VGG进行训练 epoch=3-step=1023                                          FPS=22 config\n",
    "# ./weight/StableAudio/2024-07-19 18:04:16               在basemodel的基础上 加入pos_emb 利用AS重新训练  epoch=13-step=220   <epoch=88-step=220>    FPS=22  config\n",
    "# ./weight/StableAudio/2024-07-20 13:34:51               在basemodel的基础上 加入pos_emb 利用VGG重新训练 epoch=3-step=1817                          FPS=22  config\n",
    "# ./weight/StableAudio/2024-07-22 19:08:45               在不load t2a-crosscond&conditioner的基础上 加入rotary_cond_emb 利用AS重新训练              FPS=8 sr=16000  config_rotebd\n",
    "# ./weight/StableAudio/2024-07-24 23:06:33               在bestmodel的基础上    利用AS继续进行训练     <epoch=70-step=304> <epoch=150-step=304>            FPS=22  config\n",
    "# ./weight/StableAudio/2024-08-01 09:36:20               在lastmodel<epoch=70-step=304>的基础上    利用ASVGG继续进行训练   <epoch=45-step=2818>  <epoch=29-step=2818>    FPS=22  config\n",
    "# ./weight/StableAudio/2024-08-01 09:36:20               在lastmodel<epoch=29-step=2818>的基础上    利用ASVGG继续进行训练\n",
    "#                                                        在不load t2a-crosscond&conditioner的基础上 加入rotary_cond_emb 利用VGG重新训练             FPS=8 sr=16000  config_rotebd\n",
    "#                                                        在xxxxxxxxx的基础上 加入global_cond_ids [\"time_cond\", \"seconds_total\"]，采用prepend\n",
    "#                                                        在xxxxxxxxx的基础上 加入global_cond_ids [\"time_cond\", \"seconds_total\"]，采用concat\n",
    "\n",
    "try:\n",
    "    state_dict = load_file(f'{model_dir}/{model_name}.safetensors')\n",
    "except:\n",
    "    state_dict = torch.load(f'{model_dir}/{model_name}.ckpt')['state_dict']\n",
    "    state_dict = OrderedDict([(\".\".join(key.split('.')[1:]), value)  for key, value in state_dict.items()])\n",
    "    safetensors.torch.save_file(state_dict, f'{model_dir}/{model_name}.safetensors')\n",
    "\n",
    "print(f'{model_dir}/{model_name}.safetensors')\n",
    "# state_dict = load_file(f'./weight/StableAudio/model.safetensors')\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 T2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m----> 2\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m load_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./weight/StableAudio/model.safetensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 6\n",
    "state_dict = load_file(f'./weight/StableAudio/model.safetensors')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up text and timing conditioning\n",
    "conditioning = {\n",
    "    \"prompt\": [\"laugh\"], ### 128 BPM tech house drum loop\n",
    "    # 'feature': ['/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4'],\n",
    "    \"seconds_start\": torch.tensor([0]), \n",
    "    \"seconds_total\": torch.tensor([10])\n",
    "}\n",
    "\n",
    "# Generate stereo audio\n",
    "output = generate_diffusion_cond(\n",
    "    model,\n",
    "    steps=50,\n",
    "    cfg_scale=7,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=44100*47,\n",
    "    sigma_min=0.3,\n",
    "    sigma_max=500,\n",
    "    sampler_type=\"k-dpm-fast\", # dpmpp-3m-sde\n",
    "    device=device,\n",
    "    seed=3044415654\n",
    ")\n",
    "# Rearrange audio batch to a single sequence\n",
    "output = rearrange(output, \"b d n -> d (b n)\")\n",
    "\n",
    "# Peak normalize, clip, convert to int16, and save to file\n",
    "output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "torchaudio.save(\"output.wav\", output, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 files\n"
     ]
    }
   ],
   "source": [
    "info_dirs = [\n",
    "    './dataset/feature/train/AudioSet/10', \n",
    "    # './dataset/feature/train/VGGSound/10',\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    '/home/chengxin/chengxin/AudioSet/generated_audios/train/10', \n",
    "    # '/home/chengxin/chengxin/VGGSound/generated_audios/train/10'\n",
    "    ]\n",
    "# info_dirs = ['./dataset/feature/train/AudioSet/10', './dataset/feature/train/VGGSound/10']\n",
    "# audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10', '/home/chengxin/chengxin/VGGSound/generated_audios/train/10']\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'sample_size':sample_size,\n",
    "    'fps':fps,\n",
    "    'force_channels':\"mono\",\n",
    "    'limit_num':300\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':20, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | diffusion | ConditionedDiffusionModelWrapper | 1.2 B \n",
      "1 | losses    | MultiLoss                        | 0     \n",
      "---------------------------------------------------------------\n",
      "1.1 B     Trainable params\n",
      "156 M     Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,855.713 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:00<00:00,  0.25it/s, v_num=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:30<00:00,  0.17it/s, v_num=11]\n"
     ]
    }
   ],
   "source": [
    "training_config = model_config.get('training', None)\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "            model=model, \n",
    "            lr=training_config.get(\"learning_rate\", None),\n",
    "            optimizer_configs=training_config.get(\"optimizer_configs\", None),\n",
    "            pre_encoded=training_config.get(\"pre_encoded\", False),\n",
    "            cfg_dropout_prob = training_config.get(\"cfg_dropout_prob\", 0.1),\n",
    "            timestep_sampler = training_config.get(\"timestep_sampler\", \"uniform\"),\n",
    "        )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=[1],\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "info_dirs = [\n",
    "    # './dataset/feature/test/VGGSound/10', \n",
    "    './dataset/feature/test/unav100/10'\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    # '/home/chengxin/chengxin/VGGSound/generated_audios/test/10', \n",
    "    '/home/chengxin/chengxin/unav100/generated_audios/test'\n",
    "    ]\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    # 'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'fps':fps,\n",
    "    'force_channels':\"mono\",\n",
    "    'limit_num':4\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':32, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./demo/epoch=80-step=254\n",
      "tensor([60.0100])\n",
      "1270040263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:21<00:00,  6.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_dir = './weight/StableAudio/2024-07-06 10:28:13'\n",
    "# model_name = 'epoch=30-step=58'  \n",
    "\n",
    "device = 0\n",
    "output_dir = f\"./demo/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "for conditioning in dataloader:\n",
    "    seconds_total = max(conditioning['seconds_total'])\n",
    "    \n",
    "    output = generate_diffusion_cond(\n",
    "        model = model.to(device),\n",
    "        steps=150,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=int(sample_rate*seconds_total),\n",
    "        batch_size=len(conditioning['feature']),\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"dpmpp-3m-sde\", # k-dpm-fast\"\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    for idx in range(len(conditioning['feature'])):\n",
    "        # Save generated audio\n",
    "        if 'AuidoSet' in conditioning['video_path'][idx] or 'AudioSet' in conditioning['video_path'][idx]:\n",
    "            l = conditioning['video_path'][idx].split('/')\n",
    "            video_path = os.path.join('/home/chengxin/chengxin/AudioSet/dataset/', l[-4], l[-2], l[-1])\n",
    "        else:\n",
    "            video_path = conditioning['video_path'][idx].replace('../../../', '/home/chengxin/chengxin/')\n",
    "\n",
    "        # video_path = conditioning['video_path'][idx].replace('../../', './')\n",
    "        audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "        waveform = output[idx:1+idx,...,:int(conditioning['seconds_total'][idx]*sample_rate)]\n",
    "        # print(output.shape, output[idx:idx+1].shape, waveform.shape)\n",
    "        save_audio(waveform, audio_path, sample_rate)\n",
    "        \n",
    "        # Replace the audio of original video to generated one\n",
    "        moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "        shutil.copy(video_path, moved_video_path)\n",
    "        generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "        replace_audio(moved_video_path, audio_path, generated_video_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190309694\n",
      "Extracting features from video:/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4\n",
      "Extracting features from video:/home/chengxin/chengxin/FoleyCrafter/examples/sora/1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 30.41it/s]\n"
     ]
    }
   ],
   "source": [
    "conditioning = {\n",
    "    'seconds_start': [0, 0],\n",
    "    'seconds_total': [10, 10],\n",
    "    'feature': ['/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4', '/home/chengxin/chengxin/FoleyCrafter/examples/sora/1.mp4']\n",
    "}\n",
    "\n",
    "output = generate_diffusion_cond(\n",
    "    model = model.to(device),\n",
    "    steps=50,\n",
    "    cfg_scale=7,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=int(sample_rate*seconds_total),\n",
    "    batch_size=len(conditioning['feature']),\n",
    "    sigma_min=0.3,\n",
    "    sigma_max=500,\n",
    "    sampler_type=\"k-dpm-fast\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 0\n",
    "# seconds_total = 10\n",
    "# video_paths = ['/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4', '/home/chengxin/chengxin/FoleyCrafter/examples/sora/1.mp4']\n",
    "# output_dir = \"./demo\"\n",
    "\n",
    "# conditioning = {'seconds_start':0,'seconds_total':10}\n",
    "# output = generate_diffusion_cond_from_path(\n",
    "#     model=model.to(device),\n",
    "#     video_paths=video_paths,\n",
    "#     conditioning=conditioning,\n",
    "#     steps=100,\n",
    "#     cfg_scale=7,\n",
    "#     sample_rate=sample_rate,\n",
    "#     sample_size=sample_rate*seconds_total,\n",
    "#     batch_size=len(video_paths),\n",
    "#     sigma_min=0.3,\n",
    "#     sigma_max=500,\n",
    "#     sampler_type=\"dpmpp-3m-sde\",\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# for idx in range(len(video_paths)):\n",
    "#     # Save generated audio\n",
    "#     video_path = video_paths[idx]\n",
    "#     audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "#     save_audio(output[idx:1+idx], audio_path, sample_rate)\n",
    "        \n",
    "#     # Replace the audio of original video to generated one\n",
    "#     moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "#     shutil.copy(video_path, moved_video_path)\n",
    "#     generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "#     replace_audio(moved_video_path, audio_path, generated_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        use_xpos = False,\n",
    "        scale_base = 512,\n",
    "        interpolation_factor = 1.,\n",
    "        base = 10000,\n",
    "        base_rescale_factor = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n",
    "        # has some connection to NTK literature\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n",
    "        base *= base_rescale_factor ** (dim / (dim - 2))\n",
    "\n",
    "        inv_freq = 1. / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "        assert interpolation_factor >= 1.\n",
    "        self.interpolation_factor = interpolation_factor\n",
    "\n",
    "        if not use_xpos:\n",
    "            self.register_buffer('scale', None)\n",
    "            return\n",
    "\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "\n",
    "        self.scale_base = scale_base\n",
    "        self.register_buffer('scale', scale)\n",
    "\n",
    "    def forward_from_seq_len(self, seq_len):\n",
    "        device = self.inv_freq.device\n",
    "\n",
    "        t = torch.arange(seq_len, device = device)\n",
    "        return self.forward(t)\n",
    "\n",
    "    @autocast(enabled = False)\n",
    "    def forward(self, t):\n",
    "        device = self.inv_freq.device\n",
    "\n",
    "        t = t.to(torch.float32)\n",
    "\n",
    "        t = t / self.interpolation_factor\n",
    "\n",
    "        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        freqs = torch.cat((freqs, freqs), dim = -1)\n",
    "\n",
    "        if self.scale is None:\n",
    "            return freqs, 1.\n",
    "\n",
    "        power = (torch.arange(seq_len, device = device) - (seq_len // 2)) / self.scale_base\n",
    "        scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "        scale = torch.cat((scale, scale), dim = -1)\n",
    "\n",
    "        return freqs, scale\n",
    "\n",
    "rotary_pos_emb = RotaryEmbedding(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce, partial\n",
    "def rotate_half(x):\n",
    "    x = rearrange(x, '... (j d) -> ... j d', j = 2)\n",
    "    x1, x2 = x.unbind(dim = -2)\n",
    "    return torch.cat((-x2, x1), dim = -1)\n",
    "\n",
    "def apply_rotary_pos_emb(t, freqs, scale = 1):\n",
    "    out_dtype = t.dtype\n",
    "\n",
    "    # cast to float32 if necessary for numerical stability\n",
    "    dtype = reduce(torch.promote_types, (t.dtype, freqs.dtype, torch.float32))\n",
    "    rot_dim, seq_len = freqs.shape[-1], t.shape[-2]\n",
    "\n",
    "    freqs, t = freqs.to(dtype), t.to(dtype)\n",
    "    freqs = freqs[-seq_len:, :]\n",
    "\n",
    "    if t.ndim == 4 and freqs.ndim == 3:\n",
    "        freqs = rearrange(freqs, 'b n d -> b 1 n d')\n",
    "\n",
    "\n",
    "    # partial rotary embeddings, Wang et al. GPT-J\n",
    "    t, t_unrotated = t[..., :rot_dim], t[..., rot_dim:]\n",
    "    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n",
    "\n",
    "    t, t_unrotated = t.to(out_dtype), t_unrotated.to(out_dtype)\n",
    "\n",
    "    return torch.cat((t, t_unrotated), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 5.6234e-01, 3.1623e-01,  ..., 5.6234e-04, 3.1623e-04,\n",
       "         1.7783e-04],\n",
       "        [2.0000e+00, 1.1247e+00, 6.3246e-01,  ..., 1.1247e-03, 6.3246e-04,\n",
       "         3.5566e-04],\n",
       "        ...,\n",
       "        [9.7000e+01, 5.4547e+01, 3.0674e+01,  ..., 5.4547e-02, 3.0674e-02,\n",
       "         1.7249e-02],\n",
       "        [9.8000e+01, 5.5109e+01, 3.0990e+01,  ..., 5.5109e-02, 3.0990e-02,\n",
       "         1.7427e-02],\n",
       "        [9.9000e+01, 5.5672e+01, 3.1307e+01,  ..., 5.5672e-02, 3.1307e-02,\n",
       "         1.7605e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs, _ = rotary_pos_emb.forward_from_seq_len(100)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Time Cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "info_dirs = [\n",
    "    './dataset/feature_t/test/AudioSet/10', \n",
    "    # './dataset/feature/test/unav100/10'\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    '/home/chengxin/chengxin/AudioSet/generated_audios/test/10', \n",
    "    # '/home/chengxin/chengxin/unav100/generated_audios/test'\n",
    "    ]\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    # 'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'fps':fps,\n",
    "    'force_channels':\"mono\",\n",
    "    'limit_num':32\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':32, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./demo/epoch=30-step=58\n",
      "3642395041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.92it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir = './weight/StableAudio/2024-07-06 10:28:13'\n",
    "model_name = 'epoch=30-step=58'  \n",
    "\n",
    "device = 7\n",
    "output_dir = f\"./demo/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "for conditioning in dataloader:\n",
    "    del conditioning['seconds_start']\n",
    "    seconds_total = max(conditioning['seconds_total'])\n",
    "    output = generate_diffusion_cond(\n",
    "        model = model.to(device),\n",
    "        steps=50,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=int(sample_rate*seconds_total),\n",
    "        batch_size=len(conditioning['feature']),\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"k-dpm-fast\",\n",
    "        device=device\n",
    "    )\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moviepy.editor import VideoFileClip\n",
    "# from PIL import Image\n",
    "# mp4_file = '/home/chengxin/chengxin/stable-v2a/stable_audio_tools/data/Pv6BhKDXpHE_000026.mp4'\n",
    "# video = VideoFileClip(mp4_file)\n",
    "\n",
    "\n",
    "# video.write_videofile(\"Pv6BhKDXpHE_000026.mp4\", codec='libx264') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import load_file\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "from stable_audio_tools import create_model_from_config\n",
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from stable_audio_tools.training.training_wrapper import DiffusionCondTrainingWrapper\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "\n",
    "model_config_file = './stable_audio_tools/configs/model_config.json'\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model = create_model_from_config(model_config)\n",
    "# model.load_state_dict(load_file('./weight/StableAudio/model.safetensors'), strict=False)\n",
    "# model.load_state_dict(torch.load('./lightning_logs/version_0/checkpoints/epoch=99-step=13800.ckpt')['state_dict'], strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 204665 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204665"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dirs = ['./dataset/feature/train/AudioSet/10']\n",
    "audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10']\n",
    "info_dirs = ['./dataset/feature/train/AudioSet/10', './dataset/feature/train/VGGSound/10']\n",
    "audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10', '/home/chengxin/chengxin/VGGSound/generated_audios/train/10']\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':44100, \n",
    "    'force_channels':\"stereo\"\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':10, \n",
    "    'shuffle':True,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config = model_config.get('training', None)\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "            model, \n",
    "            lr=training_config.get(\"learning_rate\", None),\n",
    "            mask_padding=training_config.get(\"mask_padding\", False),\n",
    "            mask_padding_dropout=training_config.get(\"mask_padding_dropout\", 0.0),\n",
    "            use_ema = training_config.get(\"use_ema\", True),\n",
    "            log_loss_info=training_config.get(\"log_loss_info\", False),\n",
    "            optimizer_configs=training_config.get(\"optimizer_configs\", None),\n",
    "            pre_encoded=training_config.get(\"pre_encoded\", False),\n",
    "            cfg_dropout_prob = training_config.get(\"cfg_dropout_prob\", 0.1),\n",
    "            timestep_sampler = training_config.get(\"timestep_sampler\", \"uniform\")\n",
    "        )\n",
    "\n",
    "training_wrapper.load_state_dict(torch.load('./lightning_logs/version_0/checkpoints/epoch=99-step=13800.ckpt')['state_dict'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=[1],\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35621 files\n"
     ]
    }
   ],
   "source": [
    "def replace_audio(input_video, input_audio, output_video):\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_video,        # 输入视频文件\n",
    "        '-i', input_audio,        # 输入音频文件\n",
    "        '-c:v', 'copy',           # 复制视频流，不重新编码视频\n",
    "        '-c:a', 'aac',            # 指定音频编码为aac\n",
    "        '-map', '0:v:0',          # 从第一个输入（视频）映射视频流\n",
    "        '-map', '1:a:0',          # 从第二个输入（音频）映射音频流\n",
    "        '-strict', 'experimental', # 允许使用实验性编码\n",
    "        '-loglevel', 'error',\n",
    "        output_video,              # 输出视频文件\n",
    "        '-y'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ffmpeg_cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"命令执行失败: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "info_dirs = ['./dataset/feature/test/AudioSet/10']\n",
    "audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/test/10']\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':44100, \n",
    "    'force_channels':\"stereo\"\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':10, \n",
    "    'shuffle':True,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1269009644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torchsde/_brownian/brownian_interval.py:608: UserWarning: Should have tb<=t1 but got tb=500.00006103515625 and t1=500.000061.\n",
      "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
      "100%|██████████| 100/100 [00:05<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset/video/test/AudioSet/10/xUqfBPhdLOM_p.mp4 ./demo/xUqfBPhdLOM_p.mp4 ./demo/xUqfBPhdLOM_p.wav ./demo/xUqfBPhdLOM_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/fcLK-nPc7-w_p.mp4 ./demo/fcLK-nPc7-w_p.mp4 ./demo/fcLK-nPc7-w_p.wav ./demo/fcLK-nPc7-w_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/YpjJNavdrmc_p.mp4 ./demo/YpjJNavdrmc_p.mp4 ./demo/YpjJNavdrmc_p.wav ./demo/YpjJNavdrmc_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/COoJtPFTkhU_p.mp4 ./demo/COoJtPFTkhU_p.mp4 ./demo/COoJtPFTkhU_p.wav ./demo/COoJtPFTkhU_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/SM10Hrl5d7U_p.mp4 ./demo/SM10Hrl5d7U_p.mp4 ./demo/SM10Hrl5d7U_p.wav ./demo/SM10Hrl5d7U_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/DHk6BiokRC4_p.mp4 ./demo/DHk6BiokRC4_p.mp4 ./demo/DHk6BiokRC4_p.wav ./demo/DHk6BiokRC4_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/UrUJ9tUJynA_p.mp4 ./demo/UrUJ9tUJynA_p.mp4 ./demo/UrUJ9tUJynA_p.wav ./demo/UrUJ9tUJynA_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/wRnPuL0kuXY_p.mp4 ./demo/wRnPuL0kuXY_p.mp4 ./demo/wRnPuL0kuXY_p.wav ./demo/wRnPuL0kuXY_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/EsoKL_41uOc_p.mp4 ./demo/EsoKL_41uOc_p.mp4 ./demo/EsoKL_41uOc_p.wav ./demo/EsoKL_41uOc_p_GEN.mp4\n",
      "../../dataset/video/test/AudioSet/10/6ayVxYbKBe0_p.mp4 ./demo/6ayVxYbKBe0_p.mp4 ./demo/6ayVxYbKBe0_p.wav ./demo/6ayVxYbKBe0_p_GEN.mp4\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model = training_wrapper.diffusion\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "sample_size = 441000\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "for audio, conditioning in dataloader:\n",
    "    output = generate_diffusion_cond(\n",
    "        model,\n",
    "        steps=100,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=sample_size,\n",
    "        batch_size=dl_config['batch_size'],\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"dpmpp-3m-sde\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Rearrange audio batch to a single sequence\n",
    "    idx = 0\n",
    "\n",
    "    for idx in range(dl_config['batch_size']):\n",
    "        video_file = conditioning['video_path'][idx].replace('../../', './')\n",
    "\n",
    "        generated_audio = output[idx:1+idx]\n",
    "        generated_audio = rearrange(generated_audio, \"b d n -> d (b n)\")\n",
    "        generated_audio = generated_audio.to(torch.float32).div(torch.max(torch.abs(generated_audio))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "        audio_file = f\"./demo/{video_file.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "        torchaudio.save(audio_file, generated_audio, sample_rate)\n",
    "\n",
    "        video_file_ = f\"./demo/{video_file.split('/')[-1]}\"\n",
    "        generated_video_file = video_file_.replace(\".mp4\",\"_GEN.mp4\")\n",
    "        shutil.copy(video_file, video_file_)\n",
    "        replace_audio(video_file_, audio_file, generated_video_file)\n",
    "        print(conditioning['video_path'][idx], video_file_, audio_file, generated_video_file)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dataset/video/test/AudioSet/10/--4gqARaEJE_p.mp4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditioning['video_path'][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_audio_tools.models.diffusion import DiTWrapper, ConditionedDiffusionModelWrapper\n",
    "from stable_audio_tools.models.conditioners import create_multi_conditioner_from_conditioning_config\n",
    "from stable_audio_tools.models.autoencoders import create_autoencoder_from_config\n",
    "from stable_audio_tools.models.pretransforms import AutoencoderPretransform\n",
    "\n",
    "io_channels = model_config[\"model\"].get('io_channels', None)\n",
    "sample_rate = model_config.get('sample_rate', None)\n",
    "assert io_channels is not None, \"Must specify io_channels in model config\"\n",
    "assert sample_rate is not None, \"Must specify sample_rate in config\"\n",
    "\n",
    "\n",
    "\n",
    "diffusion_config = {\n",
    "    'cross_attention_cond_ids': ['feature'],\n",
    "    'type': 'dit',\n",
    "    'config': {\n",
    "        'io_channels': 64,\n",
    "        'embed_dim': 1536,\n",
    "        'depth': 24,\n",
    "        'num_heads': 24,\n",
    "        'cond_token_dim': 768,\n",
    "        'global_cond_dim': 1536,\n",
    "        'project_cond_tokens': False,\n",
    "        'transformer_type': 'continuous_transformer'\n",
    "        }\n",
    "}\n",
    "diffusion_objective = diffusion_config.get('diffusion_objective', 'v')\n",
    "cross_attention_ids = diffusion_config.get('cross_attention_cond_ids', [])\n",
    "global_cond_ids = diffusion_config.get('global_cond_ids', [])\n",
    "input_concat_ids = diffusion_config.get('input_concat_ids', [])\n",
    "prepend_cond_ids = diffusion_config.get('prepend_cond_ids', [])\n",
    "\n",
    "diffusion_model_config = diffusion_config['config']\n",
    "diffusion_model = DiTWrapper(**diffusion_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning_config = {\n",
    "    'configs': [\n",
    "        {\n",
    "            'id': 'duration',\n",
    "            'type': 'number',\n",
    "            'config': {'min_val': 0, 'max_val': 512}\n",
    "         },\n",
    "         {\n",
    "             'id': 'feature', \n",
    "             'type': 'video_feature', \n",
    "             'config': {}\n",
    "        }\n",
    "    ],\n",
    "    'cond_dim': 768,\n",
    "    'default_keys': {'feature': 'video_path'}\n",
    "}\n",
    "conditioner = create_multi_conditioner_from_conditioning_config(conditioning_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretransform_config = {\n",
    "    'type': 'autoencoder',\n",
    "    'iterate_batch': True,\n",
    "    'config': {\n",
    "        'encoder': {\n",
    "            'type': 'oobleck',\n",
    "            'requires_grad': False,\n",
    "            'config': {\n",
    "                'in_channels': 2,\n",
    "                'channels': 128,\n",
    "                'c_mults': [1, 2, 4, 8, 16],\n",
    "                'strides': [2, 4, 4, 8, 8],\n",
    "                'latent_dim': 128,\n",
    "                'use_snake': True\n",
    "            }\n",
    "        },\n",
    "        'decoder': {\n",
    "            'type': 'oobleck',\n",
    "            'config': {\n",
    "                'out_channels': 2,\n",
    "                'channels': 128,\n",
    "                'c_mults': [1, 2, 4, 8, 16],\n",
    "                'strides': [2, 4, 4, 8, 8],\n",
    "                'latent_dim': 64,\n",
    "                'use_snake': True,\n",
    "                'final_tanh': False\n",
    "            }\n",
    "        },\n",
    "        'bottleneck': {'type': 'vae'},\n",
    "        'latent_dim': 64,\n",
    "        'downsampling_ratio': 2048,\n",
    "        'io_channels': 2\n",
    "        }\n",
    "    }\n",
    "\n",
    "autoencoder_config = {\"sample_rate\": sample_rate, \"model\": pretransform_config[\"config\"]}\n",
    "autoencoder = create_autoencoder_from_config(autoencoder_config)\n",
    "\n",
    "\n",
    "pretransform = AutoencoderPretransform(\n",
    "    autoencoder, \n",
    "    scale=pretransform_config.get(\"scale\", 1.0),\n",
    "    model_half=pretransform_config.get(\"model_half\", False), \n",
    "    iterate_batch=pretransform_config.get(\"iterate_batch\", False), \n",
    "    chunked=pretransform_config.get(\"chunked\", False)\n",
    ")\n",
    "pretransform.enable_grad = pretransform_config.get('enable_grad', False)\n",
    "pretransform.eval().requires_grad_(pretransform.enable_grad)\n",
    "\n",
    "\n",
    "min_input_length = pretransform.downsampling_ratio\n",
    "min_input_length *= diffusion_model.model.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kwargs = {\"diffusion_objective\":diffusion_objective}\n",
    "model = ConditionedDiffusionModelWrapper(\n",
    "        diffusion_model,\n",
    "        conditioner,\n",
    "        min_input_length=min_input_length,\n",
    "        sample_rate=sample_rate,\n",
    "        cross_attn_cond_ids=cross_attention_ids,\n",
    "        global_cond_ids=global_cond_ids,\n",
    "        input_concat_ids=input_concat_ids,\n",
    "        prepend_cond_ids=prepend_cond_ids,\n",
    "        pretransform=pretransform,\n",
    "        io_channels=io_channels,\n",
    "        **extra_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(160000, 16000, 10.0, None, 8, 'mono')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import safetensors\n",
    "from safetensors.torch import load_file\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_audio_tools import create_model_from_config, replace_audio, save_audio\n",
    "from stable_audio_tools.data.dataset import VideoFeatDataset, VideoFeatDataset_VL, collation_fn\n",
    "from stable_audio_tools.training.training_wrapper import DiffusionCondTrainingWrapper\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond, generate_diffusion_cond_from_path\n",
    "\n",
    "\n",
    "model_config_file = './stable_audio_tools/configs/model_config_gc16000.json'\n",
    "# model_config_file = './stable_audio_tools/configs/model_config_vl30.json'\n",
    "# model_config_file = './stable_audio_tools/configs/model_config_ss30.json'\n",
    "\n",
    "\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)\n",
    "    sample_rate = model_config[\"sample_rate\"]\n",
    "    sample_size = model_config[\"sample_size\"]\n",
    "    fps = model_config[\"fps\"]\n",
    "    variable_length = model_config[\"variable_length\"]\n",
    "    force_channels = \"stereo\" if model_config[\"audio_channels\"] == 2 else \"mono\"\n",
    "\n",
    "\n",
    "model = create_model_from_config(model_config)\n",
    "sample_size, sample_rate, sample_size/sample_rate, variable_length, fps, force_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./weight/StableAudio/2024-08-04 02:52:24/epoch=60-step=2818.safetensors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConditionedDiffusionModelWrapper:\n\tMissing key(s) in state_dict: \"conditioner.conditioners.seconds_start.embedder.embedding.0.weights\", \"conditioner.conditioners.seconds_start.embedder.embedding.1.weight\", \"conditioner.conditioners.seconds_start.embedder.embedding.1.bias\", \"conditioner.conditioners.seconds_total.embedder.embedding.0.weights\", \"conditioner.conditioners.seconds_total.embedder.embedding.1.weight\", \"conditioner.conditioners.seconds_total.embedder.embedding.1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# cond_state_dict = load_file(f'./weight/StableAudio/model.safetensors')\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# cond_state_dict = {k: v for k, v in cond_state_dict.items() if ('seconds' in k)}\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# state_dict.update(cond_state_dict)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConditionedDiffusionModelWrapper:\n\tMissing key(s) in state_dict: \"conditioner.conditioners.seconds_start.embedder.embedding.0.weights\", \"conditioner.conditioners.seconds_start.embedder.embedding.1.weight\", \"conditioner.conditioners.seconds_start.embedder.embedding.1.bias\", \"conditioner.conditioners.seconds_total.embedder.embedding.0.weights\", \"conditioner.conditioners.seconds_total.embedder.embedding.1.weight\", \"conditioner.conditioners.seconds_total.embedder.embedding.1.bias\". "
     ]
    }
   ],
   "source": [
    "import safetensors.torch\n",
    "\n",
    "\n",
    "model_dir = './weight/StableAudio/2024-08-04 02:52:24'\n",
    "model_name = 'epoch=60-step=2818'  \n",
    "# ./weight/StableAudio/lightning_logs/version_3/checkpoints  epoch=9-step=14620                                     FPS=22  config\n",
    "# ./weight/StableAudio/2024-07-06 10:28:13               在basemodel的基础上 加入pos_emb,利用AS进行训练<epoch=30-step=58>   time_align和生成效果表现较好(best)   FPS=22 config\n",
    "# ./weight/StableAudio/2024-07-20 13:34:51               在basemodel的基础上 加入pos_emb 利用VGG重新训练 epoch=3-step=1817                          FPS=22  config\n",
    "# ./weight/StableAudio/2024-07-22 19:08:45               在不load t2a-crosscond&conditioner的基础上 加入rotary_cond_emb 利用AS重新训练              FPS=8 sr=16000  config_rotebd\n",
    "# ./weight/StableAudio/2024-07-24 23:06:33               在bestmodel的基础上    利用AS继续进行训练     <epoch=70-step=304> <epoch=150-step=304>     FPS=22  config\n",
    "# ./weight/StableAudio/2024-08-01 09:36:20               在lastmodel<epoch=70-step=304>的基础上    利用ASVGG继续进行训练   <epoch=45-step=2818>  <epoch=29-step=2818>    FPS=22  config\n",
    "# ./weight/StableAudio/2024-08-04 02:52:24               在lastmodel<epoch=29-step=2818>的基础上    利用ASVGG继续进行训练  <epoch=27-step=2818> <epoch=36-step=2818> <epoch=60-step=2818>\n",
    "# ./weight/StableAudio/2024-08-15 11:31:46               在lastmodel<epoch=36-step=2818>的基础上 加入global_cond_ids [\"seconds_start\", \"seconds_total\"] samplesize44100*30 variablelength30\n",
    "# ./weight/StableAudio/2024-08-19 11:11:56               在lastmodel<epoch=36-step=2818>, sr16000 ss160000 fps8 noclamp  加入global_cond_ids 利用VGG继续进行训练   <epoch=60-step=2818> <epoch=30-step=2818> <epoch=35-step=2818> \n",
    "\n",
    "try:\n",
    "    state_dict = load_file(f'{model_dir}/{model_name}.safetensors')\n",
    "except:\n",
    "    state_dict = torch.load(f'{model_dir}/{model_name}.ckpt')['state_dict']\n",
    "    state_dict = OrderedDict([(\".\".join(key.split('.')[1:]), value)  for key, value in state_dict.items()])\n",
    "    safetensors.torch.save_file(state_dict, f'{model_dir}/{model_name}.safetensors')\n",
    "\n",
    "print(f'{model_dir}/{model_name}.safetensors')\n",
    "\n",
    "# cond_state_dict = load_file(f'./weight/StableAudio/model.safetensors')\n",
    "# cond_state_dict = {k: v for k, v in cond_state_dict.items() if ('seconds' in k)}\n",
    "# state_dict.update(cond_state_dict)\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dirs = [\n",
    "    # './dataset/feature/train/AudioSet/10', \n",
    "    './dataset/feature/train/VGGSound/10',\n",
    "    # './dataset/feature/train/unav100/10',\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    # '/home/chengxin/chengxin/AudioSet/generated_audios/train/10', \n",
    "    '/home/chengxin/chengxin/VGGSound/generated_audios/train/10'\n",
    "    # '/home/chengxin/chengxin/unav100/generated_audios/train/10',\n",
    "    ]\n",
    "# info_dirs = ['./dataset/feature/train/AudioSet/10', './dataset/feature/train/VGGSound/10']\n",
    "# audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10', '/home/chengxin/chengxin/VGGSound/generated_audios/train/10']\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'sample_size':sample_size,\n",
    "    'variable_length':variable_length,\n",
    "    'fps':fps,\n",
    "    'force_channels':force_channels,\n",
    "    'limit_num':7000\n",
    "}\n",
    "\n",
    "\n",
    "dl_config = {\n",
    "    'batch_size':4, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset_VL(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 10, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, info = dataset[4]\n",
    "info['feature'].shape[0]//fps, info['seconds_total'], audio.shape[1]//sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | diffusion | ConditionedDiffusionModelWrapper | 1.2 B \n",
      "1 | losses    | MultiLoss                        | 0     \n",
      "---------------------------------------------------------------\n",
      "1.1 B     Trainable params\n",
      "156 M     Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,880.998 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s, v_num=5]Epoch:0   Loss:0.7678540945053101\n"
     ]
    }
   ],
   "source": [
    "training_config = model_config.get('training', None)\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "            model=model, \n",
    "            lr=training_config.get(\"learning_rate\", None),\n",
    "            optimizer_configs=training_config.get(\"optimizer_configs\", None),\n",
    "            pre_encoded=training_config.get(\"pre_encoded\", False),\n",
    "            cfg_dropout_prob = training_config.get(\"cfg_dropout_prob\", 0.1),\n",
    "            timestep_sampler = training_config.get(\"timestep_sampler\", \"uniform\"),\n",
    "            # duration_mask=None,\n",
    "            latent_per_sec = sample_rate // 2000\n",
    "        )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=[7],\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=1\n",
    ")\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "info_dirs = [\n",
    "    './dataset/feature/test/VGGSound/10', \n",
    "    # './dataset/feature/test/unav100/10'\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    '/home/chengxin/chengxin/VGGSound/generated_audios/test/10', \n",
    "    # '/home/chengxin/chengxin/unav100/generated_audios/test/10'\n",
    "    ]\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'sample_size':sample_size,\n",
    "    'variable_length':variable_length,\n",
    "    'fps':fps,\n",
    "    # 'force_channels':\"mono\",\n",
    "    'limit_num':4\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':4, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset_VL(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./demo/epoch=36-step=2818\n",
      "2653045574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:10<00:00, 14.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_dir = './weight/StableAudio/2024-07-06 10:28:13'\n",
    "# model_name = 'epoch=30-step=58'  \n",
    "\n",
    "device = 0\n",
    "output_dir = f\"./demo/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "for audio, conditioning in dataloader:\n",
    "    # print(int(sample_rate*seconds_total), 44100*60)\n",
    "    sample_size_ = int(sample_rate*max(conditioning['seconds_total'])) if sample_size == None else sample_size\n",
    "    output = generate_diffusion_cond(\n",
    "        model = model.to(device),\n",
    "        steps=150,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=sample_size_,\n",
    "        batch_size=len(conditioning['feature']),\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"dpmpp-3m-sde\", # k-dpm-fast\"\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    for idx in range(len(conditioning['feature'])):\n",
    "        # Save generated audio\n",
    "        if 'AuidoSet' in conditioning['video_path'][idx] or 'AudioSet' in conditioning['video_path'][idx]:\n",
    "            l = conditioning['video_path'][idx].split('/')\n",
    "            video_path = os.path.join('/home/chengxin/chengxin/AudioSet/dataset/', l[-4], l[-2], l[-1])\n",
    "        else:\n",
    "            video_path = conditioning['video_path'][idx].replace('../../../', '/home/chengxin/chengxin/')\n",
    "\n",
    "        # video_path = conditioning['video_path'][idx].replace('../../', './')\n",
    "        audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "        audio_path = f\"./asset/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "        waveform = output[idx:1+idx,...,:int(conditioning['seconds_total'][idx]*sample_rate)]\n",
    "        # print(output.shape, output[idx:idx+1].shape, waveform.shape, conditioning['seconds_total'][idx])\n",
    "        save_audio(waveform, audio_path, sample_rate)\n",
    "        \n",
    "        # Replace the audio of original video to generated one\n",
    "        moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "        shutil.copy(video_path, moved_video_path)\n",
    "        generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "        replace_audio(moved_video_path, audio_path, generated_video_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2692680979\n",
      "Extracting features from video:/home/chengxin/chengxin/VGGSound/dataset/test/10/__2MwJ2uHu0_000004.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:04<00:00, 34.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "device = 0\n",
    "conditioning = {\n",
    "    'seconds_start': [0],\n",
    "    'seconds_total': [10],\n",
    "    'feature': ['/home/chengxin/chengxin/VGGSound/dataset/test/10/__2MwJ2uHu0_000004.mp4']\n",
    "}\n",
    "video_clip = VideoFileClip(conditioning['feature'][0])\n",
    "seconds_total = int(video_clip.duration)\n",
    "seconds_total = 10\n",
    "\n",
    "output = generate_diffusion_cond(\n",
    "    model = model.to(device),\n",
    "    steps=150,\n",
    "    cfg_scale=7,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=int(sample_rate*seconds_total),\n",
    "    batch_size=len(conditioning['feature']),\n",
    "    sigma_min=0.3,\n",
    "    sigma_max=500,\n",
    "    sampler_type=\"dpmpp-3m-sde\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "audio_path = \"./test.wav\"\n",
    "waveform = output[0:1,...,:int(seconds_total*sample_rate)]\n",
    "# print(output.shape, output[idx:idx+1].shape, waveform.shape)\n",
    "save_audio(waveform, audio_path, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 0\n",
    "# seconds_total = 10\n",
    "# video_paths = ['/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4', '/home/chengxin/chengxin/FoleyCrafter/examples/sora/1.mp4']\n",
    "# output_dir = \"./demo\"\n",
    "\n",
    "# conditioning = {'seconds_start':0,'seconds_total':10}\n",
    "# output = generate_diffusion_cond_from_path(\n",
    "#     model=model.to(device),\n",
    "#     video_paths=video_paths,\n",
    "#     conditioning=conditioning,\n",
    "#     steps=100,\n",
    "#     cfg_scale=7,\n",
    "#     sample_rate=sample_rate,\n",
    "#     sample_size=sample_rate*seconds_total,\n",
    "#     batch_size=len(video_paths),\n",
    "#     sigma_min=0.3,\n",
    "#     sigma_max=500,\n",
    "#     sampler_type=\"dpmpp-3m-sde\",\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# for idx in range(len(video_paths)):\n",
    "#     # Save generated audio\n",
    "#     video_path = video_paths[idx]\n",
    "#     audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "#     save_audio(output[idx:1+idx], audio_path, sample_rate)\n",
    "        \n",
    "#     # Replace the audio of original video to generated one\n",
    "#     moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "#     shutil.copy(video_path, moved_video_path)\n",
    "#     generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "#     replace_audio(moved_video_path, audio_path, generated_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Time Cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "info_dirs = [\n",
    "    './dataset/feature_t/test/AudioSet/10', \n",
    "    # './dataset/feature/test/unav100/10'\n",
    "    ]\n",
    "audio_dirs = [\n",
    "    '/home/chengxin/chengxin/AudioSet/generated_audios/test/10', \n",
    "    # '/home/chengxin/chengxin/unav100/generated_audios/test'\n",
    "    ]\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    # 'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'fps':fps,\n",
    "    'force_channels':\"mono\",\n",
    "    'limit_num':32\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':32, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./demo/epoch=30-step=58\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_dir)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conditioning \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m conditioning[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds_start\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m     seconds_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(conditioning[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds_total\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "model_dir = './weight/StableAudio/2024-07-06 10:28:13'\n",
    "model_name = 'epoch=30-step=58'  \n",
    "\n",
    "device = 7\n",
    "output_dir = f\"./demo/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "for conditioning in dataloader:\n",
    "    del conditioning['seconds_start']\n",
    "    seconds_total = max(conditioning['seconds_total'])\n",
    "    output = generate_diffusion_cond(\n",
    "        model = model.to(device),\n",
    "        steps=50,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=int(sample_rate*seconds_total),\n",
    "        batch_size=len(conditioning['feature']),\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"k-dpm-fast\",\n",
    "        device=str(device)\n",
    "    )\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import load_file\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import safetensors\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from stable_audio_tools import create_model_from_config, replace_audio, save_audio\n",
    "from stable_audio_tools.data.dataset import VideoFeatDataset, collation_fn\n",
    "from stable_audio_tools.training.training_wrapper import DiffusionCondTrainingWrapper\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond, generate_diffusion_cond_from_path\n",
    "\n",
    "\n",
    "model_config_file = './stable_audio_tools/configs/model_config.json'\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "model = create_model_from_config(model_config)\n",
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = './weight/StableAudio/2024-07-09 12:11:40'\n",
    "model_name = 'epoch=3-step=1018'  \n",
    "# ./weight/StableAudio/lightning_logs/version_3/checkpoints         epoch=9-step=14620    \n",
    "# ./weight/StableAudio/2024-07-02 19:49:04                  epoch=2-step=427\n",
    "# ./weight/StableAudio/2024-07-03 15:16:01               可训练权重attn_bias之后，epoch=5 epoch=9 出现了一些time align的现象\n",
    "# ./weight/StableAudio/2024-07-03 21:46:54               可训练权重attn_bias之后，epoch=18 多物体表现较好\n",
    "# ./weight/StableAudio/2024-07-04 11:41:24               不可训练的从0.5-1的attn_bias权重，epoch=39-step=59  epoch=3-step=55  epoch=21-step=57\n",
    "# ./weight/StableAudio/2024-07-06 10:28:13               加入pos_emb, epoch=30-step=58   time_align和生成效果表现较好(best)\n",
    "# ./weight/StableAudio/2024-07-06 21:34:56               在上一行的基础上加入VGG进行训练 epoch=3-step=1023\n",
    "# ./weight/StableAudio/2024-07-09 12:11:40               epoch=3 epoch=2 epoch=1 好像都还行\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    state_dict = load_file(f'{model_dir}/{model_name}.safetensors')\n",
    "except:\n",
    "    state_dict = torch.load(f'{model_dir}/{model_name}.ckpt')['state_dict']\n",
    "    state_dict = OrderedDict([(\".\".join(key.split('.')[1:]), value)  for key, value in state_dict.items()])\n",
    "    safetensors.torch.save_file(state_dict, f'{model_dir}/{model_name}.safetensors')\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22050 files\n"
     ]
    }
   ],
   "source": [
    "info_dirs = ['./dataset/feature/train/AudioSet/10']\n",
    "audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10']\n",
    "# info_dirs = ['./dataset/feature/train/AudioSet/10', './dataset/feature/train/VGGSound/10']\n",
    "# audio_dirs = ['/home/chengxin/chengxin/AudioSet/generated_audios/train/10', '/home/chengxin/chengxin/VGGSound/generated_audios/train/10']\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'force_channels':\"stereo\"\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':25, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA H100 PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | diffusion | ConditionedDiffusionModelWrapper | 1.2 B \n",
      "1 | losses    | MultiLoss                        | 0     \n",
      "---------------------------------------------------------------\n",
      "1.1 B     Trainable params\n",
      "156 M     Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,880.205 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 11/882 [00:40<53:34,  0.27it/s, v_num=5] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/selectors.py\", line 468, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "  File \"/home/chengxin/chengxin/anaconda3/envs/stableaudio/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 2913278) is killed by signal: Terminated. \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "training_config = model_config.get('training', None)\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "            model=model, \n",
    "            lr=training_config.get(\"learning_rate\", None),\n",
    "            optimizer_configs=training_config.get(\"optimizer_configs\", None),\n",
    "            pre_encoded=training_config.get(\"pre_encoded\", False),\n",
    "            cfg_dropout_prob = training_config.get(\"cfg_dropout_prob\", 0.1),\n",
    "            timestep_sampler = training_config.get(\"timestep_sampler\", \"uniform\"),\n",
    "        )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=[1],\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = 1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "trainer.fit(training_wrapper, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "info_dirs = ['./dataset/feature/test/VGGSound/10']\n",
    "# audio_dirs = ['/home/chengxin/chengxin/VGGSound/generated_audios/test/10']\n",
    "\n",
    "ds_config = {\n",
    "    'info_dirs' : info_dirs,\n",
    "    # 'audio_dirs' : audio_dirs,\n",
    "    'exts':'wav',\n",
    "    'sample_rate':sample_rate, \n",
    "    'force_channels':\"stereo\",\n",
    "    'limit_num':50\n",
    "}\n",
    "dl_config = {\n",
    "    'batch_size':25, \n",
    "    'shuffle':False,\n",
    "    'num_workers':4, \n",
    "    'persistent_workers':True, \n",
    "    'pin_memory':True, \n",
    "    'drop_last':False, \n",
    "}\n",
    "\n",
    "\n",
    "dataset = VideoFeatDataset(**ds_config)\n",
    "dataloader = DataLoader(dataset=dataset,  collate_fn=collation_fn, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./demo/epoch=3-step=1018\n",
      "1304458688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:20<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3820658675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:19<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "duration = 10  # generate 10 secs\n",
    "output_dir = f\"./demo/{model_name}\"\n",
    "count = 0\n",
    "max_iter = 4\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "for conditioning in dataloader:\n",
    "    output = generate_diffusion_cond(\n",
    "        model = model.to(device),\n",
    "        steps=150,\n",
    "        cfg_scale=7,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=sample_rate*duration,\n",
    "        batch_size=len(conditioning['feature']),\n",
    "        sigma_min=0.3,\n",
    "        sigma_max=500,\n",
    "        sampler_type=\"dpmpp-3m-sde\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    \n",
    "    for idx in range(len(conditioning['feature'])):\n",
    "        # Save generated audio\n",
    "        video_path = conditioning['video_path'][idx].replace('../../', './')\n",
    "        audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "        save_audio(output[idx:1+idx], audio_path, sample_rate)\n",
    "        \n",
    "        # Replace the audio of original video to generated one\n",
    "        moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "        shutil.copy(video_path, moved_video_path)\n",
    "        generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "        replace_audio(moved_video_path, audio_path, generated_video_path)\n",
    "        \n",
    "    count += 1\n",
    "    if count >= max_iter:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 0\n",
    "# duration = 10  # generate 10 secs\n",
    "# output_dir = \"./demo\"\n",
    "\n",
    "\n",
    "# for audio, conditioning in dataloader:\n",
    "#     for idx in range(dl_config['batch_size']):\n",
    "#         # Save generated audio\n",
    "#         video_path = conditioning['video_path'][idx].replace('../../', './')\n",
    "#         audio_path = f\"/home/chengxin/chengxin/AudioSet/generated_audios/specvqgan/10/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "            \n",
    "#         # Replace the audio of original video to generated one\n",
    "#         moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "#         shutil.copy(video_path, moved_video_path)\n",
    "#         generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "#         replace_audio(moved_video_path, audio_path, generated_video_path)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592135618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from video:/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4\n",
      "Extracting features from video:/home/chengxin/chengxin/FoleyCrafter/examples/sora/1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 33.39it/s]\n"
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "duration = 10\n",
    "video_paths = ['/home/chengxin/chengxin/FoleyCrafter/examples/sora/0.mp4', '/home/chengxin/chengxin/FoleyCrafter/examples/sora/1.mp4']\n",
    "output_dir = \"./demo\"\n",
    "\n",
    "output = generate_diffusion_cond_from_path(\n",
    "    model=model.to(device),\n",
    "    video_paths=video_paths,\n",
    "    steps=100,\n",
    "    cfg_scale=7,\n",
    "    sample_rate=sample_rate,\n",
    "    sample_size=sample_rate*duration,\n",
    "    batch_size=len(video_paths),\n",
    "    sigma_min=0.3,\n",
    "    sigma_max=500,\n",
    "    sampler_type=\"dpmpp-3m-sde\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "for idx in range(len(video_paths)):\n",
    "    # Save generated audio\n",
    "    video_path = video_paths[idx]\n",
    "    audio_path = f\"{output_dir}/{video_path.split('/')[-1].replace('.mp4', '.wav')}\"\n",
    "    save_audio(output[idx:1+idx], audio_path, sample_rate)\n",
    "        \n",
    "    # Replace the audio of original video to generated one\n",
    "    moved_video_path = f\"{output_dir}/{video_path.split('/')[-1]}\"\n",
    "    shutil.copy(video_path, moved_video_path)\n",
    "    generated_video_path = moved_video_path.replace(\".mp4\",\"_GEN.mp4\")\n",
    "    replace_audio(moved_video_path, audio_path, generated_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
